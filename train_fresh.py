"""
Fresh Start Training - Clean RLGym with Real Rocket League Physics
This uses RocketSim which perfectly matches Rocket League physics
"""

import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import gymnasium as gym
from gymnasium import spaces

print("="*70)
print("üöÄ FLAMEWALL - FRESH START TRAINING")
print("="*70)
print()

# Import RLGym with RocketSim (real RL physics)
print("Loading RLGym with RocketSim physics...")
import rlgym.api
from rlgym.rocket_league.sim import RocketSimEngine
from rlgym.rocket_league.action_parsers import LookupTableAction
from rlgym.rocket_league.done_conditions import GoalCondition, TimeoutCondition, AnyCondition
from rlgym.rocket_league.obs_builders import DefaultObs
from rlgym.rocket_league.reward_functions import CombinedReward, GoalReward, TouchReward
from rlgym.rocket_league.state_mutators import MutatorSequence, FixedTeamSizeMutator, KickoffMutator

print("‚úì All imports successful")
print()

print("="*70)
print("ENVIRONMENT VERIFICATION")
print("="*70)
print("‚úì Physics Engine: RocketSim (100% accurate Rocket League physics)")
print("‚úì Ball Physics: Real Rocket League ball physics")
print("‚úì Car Physics: Real Rocket League car physics")  
print("‚úì Boost: Real boost mechanics (100 pads, small/large boost)")
print("‚úì Goals: Standard Rocket League goals")
print("‚úì Field: Standard Rocket League field dimensions")
print("‚úì Game Speed: Configurable (training can be faster)")
print()


class GymnasiumWrapper(gym.Env):
    """Wraps RLGym as Gymnasium environment for Stable-Baselines3"""
    
    def __init__(self, rlgym_env):
        super().__init__()
        self.env = rlgym_env
        
        # Get initial observation to determine space
        obs_dict = self.env.reset()
        agent_id = list(obs_dict.keys())[0]
        first_obs = obs_dict[agent_id]
        
        # Define spaces based on RLGym
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=first_obs.shape,
            dtype=np.float32
        )
        
        # LookupTableAction = 90 discrete actions
        self.action_space = spaces.Discrete(90)
        
        self.agent_id = agent_id
        
    def reset(self, seed=None, options=None):
        obs_dict = self.env.reset()
        return obs_dict[self.agent_id], {}
    
    def step(self, action):
        # RLGym expects dict of actions, action needs to be numpy array
        actions = {self.agent_id: np.array([action], dtype=np.int64)}
        obs_dict, reward_dict, done_dict, truncated_dict, info = self.env.step(actions)
        
        obs = obs_dict[self.agent_id]
        reward = reward_dict[self.agent_id]
        done = done_dict[self.agent_id]
        truncated = truncated_dict[self.agent_id]
        
        return obs, reward, done, truncated, info


def create_rlgym_env():
    """Create clean RLGym environment with real RL physics"""
    print("Creating RLGym environment...")
    
    # RocketSim = Real Rocket League physics
    engine = RocketSimEngine()
    print("  ‚úì Physics: RocketSim (real RL)")
    
    # Rewards: Goals + ball touches
    reward_fn = CombinedReward(
        (GoalReward(), 10.0),     # Big reward for scoring
        (TouchReward(), 0.5),     # Small reward for touches
    )
    print("  ‚úì Rewards: Goals (10.0) + Touches (0.5)")
    
    # End episode on goal or 10 second timeout
    done_condition = AnyCondition(
        GoalCondition(),
        TimeoutCondition(timeout_seconds=10.0)
    )
    print("  ‚úì Episode ends: Goal or 10 seconds")
    
    # Standard observation (492 dimensions with padding)
    obs_builder = DefaultObs(zero_padding=11)
    print("  ‚úì Observation: DefaultObs (492 dims)")
    
    # Standard action space (90 discrete actions)
    action_parser = LookupTableAction()
    print("  ‚úì Actions: LookupTable (90 discrete)")
    
    # State mutator: Start from kickoff
    state_mutator = MutatorSequence(
        FixedTeamSizeMutator(blue_size=1, orange_size=1),  # 1v1
        KickoffMutator()  # Always start from kickoff
    )
    print("  ‚úì Team size: 1v1")
    print("  ‚úì Starting state: Kickoff")
    
    # Create environment
    env = rlgym.api.RLGym(
        state_mutator=state_mutator,
        obs_builder=obs_builder,
        action_parser=action_parser,
        reward_fn=reward_fn,
        termination_cond=done_condition,
        truncation_cond=done_condition,
        transition_engine=engine,
        renderer=None  # No rendering = faster training
    )
    
    print("\n‚úì RLGym environment created!")
    
    # Wrap for Stable-Baselines3
    env = GymnasiumWrapper(env)
    print(f"  Action space: {env.action_space}")
    print(f"  Observation space: {env.observation_space}")
    
    return env


def train():
    """Main training function"""
    print()
    print("="*70)
    print("TRAINING SETUP")
    print("="*70)
    
    # Create directories
    models_dir = "models/fresh_start"
    logs_dir = "logs/fresh_start"
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    print(f"‚úì Models will be saved to: {models_dir}")
    print(f"‚úì Logs will be saved to: {logs_dir}")
    
    # Create environment
    print()
    env = create_rlgym_env()
    
    # Check for existing model
    existing_models = [f for f in os.listdir(models_dir) if f.endswith('.zip')]
    if existing_models:
        print()
        print(f"‚ö†Ô∏è  Found {len(existing_models)} existing model(s) in {models_dir}")
        print("   DELETE THEM? This will start completely fresh!")
        response = input("   Delete existing models? (yes/no): ")
        if response.lower() == 'yes':
            for model_file in existing_models:
                os.remove(os.path.join(models_dir, model_file))
                print(f"   ‚úì Deleted {model_file}")
            print("   ‚úì All old models deleted - starting fresh!")
        else:
            print("   Keeping existing models. Training will continue from latest.")
    
    # Create PPO model
    print()
    print("Creating PPO model...")
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=logs_dir,
        learning_rate=5e-5,      # Good default for RL
        n_steps=4096,            # Steps per update
        batch_size=256,          # Batch size for training
        n_epochs=10,             # Epochs per update
        gamma=0.99,              # Discount factor
        gae_lambda=0.95,         # GAE parameter
        clip_range=0.2,          # PPO clipping
        ent_coef=0.01,           # Entropy bonus
        device="cuda",           # Use GPU if available
    )
    print("‚úì PPO model created")
    print(f"  Device: {model.device}")
    print(f"  Policy: MlpPolicy")
    print(f"  Learning rate: 5e-5")
    
    # Checkpoint callback - save every 50K steps
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000,
        save_path=models_dir,
        name_prefix="fresh_start",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    print()
    print("="*70)
    print("STARTING TRAINING")
    print("="*70)
    print()
    print("Training configuration:")
    print(f"  Total steps: 1,000,000 (1M)")
    print(f"  Save frequency: Every 50,000 steps")
    print(f"  Physics: Real Rocket League (RocketSim)")
    print(f"  Mode: 1v1 from kickoff")
    print()
    print("The bot will learn:")
    print("  ‚úì How to drive and control the car")
    print("  ‚úì How to hit the ball")
    print("  ‚úì How to score goals")
    print("  ‚úì Basic positioning and strategy")
    print()
    print("Press Ctrl+C to stop training at any time")
    print("Models are saved automatically every 50K steps")
    print()
    print("="*70)
    print()
    
    try:
        # Train for 1M steps
        model.learn(
            total_timesteps=1_000_000,
            callback=checkpoint_callback,
            progress_bar=True,
        )
        
        # Save final model
        final_path = os.path.join(models_dir, "fresh_start_final.zip")
        model.save(final_path)
        print()
        print("="*70)
        print("‚úì TRAINING COMPLETE!")
        print("="*70)
        print(f"Final model saved to: {final_path}")
        
    except KeyboardInterrupt:
        print()
        print("="*70)
        print("Training stopped by user")
        print("="*70)
        print("Latest model was already saved by checkpoint callback")
    
    env.close()


if __name__ == "__main__":
    train()
